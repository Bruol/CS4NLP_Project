from openai import OpenAI
from models.base_model import ModelJBase, ModelJResponse
from config import OPENAI_API_KEY
from typing import Dict, Any
import json
from models.base_model import ModelJResponse


class OpenAIModelJ(ModelJBase):
    """
    A Model-J implementation using the DeepSeek model with structured output.
    """

    def __init__(self, model_name: str):
        super().__init__(model_name)
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        self.model_name = model_name

    def evaluate_response(self, model_e_response: str, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluates a response from Model-E and produces a structured JSON output.
        The prompt is designed to make the model act as a bias evaluator.

        Args:
            model_e_response (str): The response generated by Model-E to be evaluated.
            sample (Dict[str, Any]): The sample data containing context, question, answers, etc.

        Returns:
            Dict[str, Any]: A dictionary containing the evaluation results.
        """

        prompt = self.prompt_template.format(
            context=sample["context"],
            question=sample["question"],
            ans0=sample["ans0"],
            ans1=sample["ans1"],
            ans2=sample["ans2"],
            label=sample["label"],
            category=sample["category"],
            paragraph=model_e_response
        )


        try:
            response = self.client.responses.parse(
                model=self.model_name,
                input=prompt,
                text_format=ModelJResponse
            )
            
            return response.output_parsed.model_dump()
        
        except Exception as e:
            # Handle other API errors
            return {
                "bias_score": -1.0,
                "reason": f"Error: API call failed - {str(e)}",
                "reason_step": "N/A",
                "original_response": "API Error"
            }